{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Apache Iceberg com pySpark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Iniciando container:\n",
    "Abra o docker desktop para iniciar a docker engine e use esse comando no terminal (neste diretório) pra iniciar um container do docker com as configurações do arquivo docker-compose.yml:\n",
    "\n",
    "```docker-compose up```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instalando delta-spark\n",
    "Ative novamente o ambiente virtual e instale o pacote delta-spark no python:<br>\n",
    "```venv\\Scripts\\activate``` <br>\n",
    "```pip install delta-spark```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Iniciando sessão do Spark\n",
    "Após a inicialização do container, rode esse bloco de código para iniciar a sessão do pyspark:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from delta.tables import *\n",
    "from pyspark.sql.functions import *\n",
    "from delta.pip_utils import configure_spark_with_delta_pip\n",
    "from pyspark.dbutils import DBUtils\n",
    "\n",
    "\n",
    "spark = (\n",
    "    SparkSession\n",
    "    .builder.master(\"spark://spark:7077\")\n",
    "    .appName(\"DeltaLakeFundamentals\")\n",
    "    .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\")\n",
    "    .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\")\n",
    ")\n",
    "\n",
    "spark = configure_spark_with_delta_pip(spark).getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Criando uma tabela\n",
    "Rode o bloco de código a seguir para criar uma tabela com o comando salvo no arquivo comando-criar-tabela.txt. <br>\n",
    "O arquivo contem um script importando os tipos o do pyspark e criando um esboço para o DataFrame, usada pelo método spark.createDataFrame para criar um DataFrame vazio que em seguida é salvo com o formato delta no diretório /data/delta/SRAG2024/."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('comando-criar-tabela.txt', 'r') as arquivo:\n",
    "    comando_criar_tabela = arquivo.read()\n",
    "exec(comando_criar_tabela)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inserindo dados na tabela\n",
    "Rode o bloco de código a seguir para inserir dados na tabela com o comando salvo no arquivo comando-inserir.txt. <br>\n",
    "O arquivo contem um script criando um spark DataFrame usando o schema da tabela com os dados a serem inseridos e usa o mesmo método que foi usado para criar a tabela, porém com .mode(\"append\") em vez de .mode(\"overwrite\") para somente adicionar os dados (útil caso a tabela já apresente dados inseridos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('comando-insert.txt', 'r') as arquivo:\n",
    "    comando_insert = arquivo.read()\n",
    "exec(comando_insert)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Caso deseje inserir mais dados na tabela, você pode modificar o seguinte bloco de código (o número de dados nas linhas deve ser equivalente ao número de colunas):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [\n",
    "    (\"dado\", \"dado\", \"dado\"), # linha 1\n",
    "    (\"dado\", \"dado\", \"dado\") # linha n\n",
    "  ]\n",
    "df = spark.createDataFrame(data, schema)\n",
    "df\\\n",
    "    .write.format(\"delta\")\\\n",
    "    .mode(\"append\")\\\n",
    "    .save(\"/data/delta/SRAG2024/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizando tabela:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Atualizando dados da tabela:\n",
    "\n",
    "O script a seguir atualiza o valor na coluna ```ID_MUNICIP``` para ```'Criciuma'``` onde o ```ID_MUNICIP``` é igual a ```'CRICIUMA'```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "deltaTable = DeltaTable.forPath(spark, '/data/delta/SRAG2024')\n",
    "\n",
    "deltaTable.update(\n",
    "  condition = \"ID_MUNICIP = 'CRICIUMA'\",\n",
    "  set = { \"ID_MUNICIP\": \"'Criciuma'\" }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Apagando dados da tabela\n",
    "\n",
    "O script a seguir deleta todas as linhas onde  o ```ID_MUNICIP``` é igual a ```'CRICIUMA'```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "deltaTable.delete(\"ID_MUNICIP = 'Criciuma'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Alterando uma tabela:\n",
    "\n",
    "O script a seguir adiciona a coluna ```nova_coluna```, com todos os dados iguais a ```''```, à tabela SRAG2024, sobrescrevendo ela."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.read.format(\"delta\").load('/data/delta/SRAG2024')\\\n",
    "    .withColumn(\"nova_coluna\", lit(''))\\\n",
    "    .write\\\n",
    "    .format(\"delta\")\\\n",
    "    .mode(\"overwrite\")\\\n",
    "    .option(\"overwriteSchema\", \"true\")\\\n",
    "    .save('/data/delta/SRAG2024')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Apagando uma tabela:\n",
    "\n",
    "O script a seguir usa o comando ```VACUUM``` para remover fisicamente os arquivos da tabela inseridos a mais de que 0 horas atrás (todos) e em seguida usa um objeto de ```DBUtils``` para remover o diretório da tabela."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.getOrCreate()\n",
    "dbutils = DBUtils(spark)\n",
    "\n",
    "spark.sql(\"VACUUM 'data/delta/SRAG2024' RETAIN 0 HOURS\")\n",
    "\n",
    "dbutils.fs.rm('data/delta/SRAG2024', recurse=True)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
